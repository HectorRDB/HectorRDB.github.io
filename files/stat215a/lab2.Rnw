\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

%For headers
\usepackage{fancyhdr}
\pagestyle{fancy}

%For figures
\usepackage{graphicx}
\graphicspath{ {figure/} }
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage[list=true,listformat=simple]{subcaption}
\usepackage[justification=centering]{caption}

%For text and equations
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\setlength{\parindent}{0pt}

\setlength{\parskip}{\smallskipamount}

\begin{document}

\title{Lab 2 - Linguistic Survey\\
Stat 215A, Fall 2017}

\author{Hector Roux de B\'{e}zieux}

\maketitle

<<setup, echo = FALSE, message=FALSE, warning=FALSE>>=
# load in useful packages
library(tidyverse)
library(forcats)
library(xtable)
library(lubridate)
library(stringr)
library(RColorBrewer)
library(cluster)
library(mgcv)

# load in the loadData() functions for Part I
source("R/load.R")
# load in the cleanData() functions for Part II
source("R/clean.R")
#Load a function for plotting multiple ggplots in the same figure
source("R/multiplot.R")
set.seed(0)
#Create the figure folder
if (!file.exists("figure")){dir.create(file.path("figure"))}
@

\part{Kernel density plots and smoothing}

<<load-data and clean it, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
# load the redwood sensor data
redwood_all_orig <- loadRedwoodData(path = "data/", source = "all")
# clean the redwood sensor data
redwood_all<-cleanRedwoodData(redwood_all_orig)
rm(redwood_all_orig)

#Filter outliers as in Lab 1
#Remove outliers
redwood_all<-redwood_all%>%drop_na()

#Remove impossible values from a physical point of vue
redwood_all<-redwood_all%>%filter(humidity>-100 &temp>0)
redwood_all<-redwood_all%>%filter(IPar>=RPar)

#Remove values where the voltage probably died
redwood_all<-redwood_all%>%filter(temp<100)

@

\section{Kernel plots for the temperature}

<<Temp,echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide',cache=TRUE>>=
#make plots with varying bandwidth, 
#store them in a list and plot them all
bw <- c(0.001,0.01,0.1,0.5,1,2,5,10)
plots <- list()

for (i in 1:length(bw)){
      plots[[i]] <- ggplot(redwood_all, aes(x = temp)) +
                    geom_density(bw = bw[i]) + theme_bw() +
                    ylab("") + xlab("") +
                    scale_x_continuous(limits = c(0,40)) +
                    ggtitle(paste("bw", bw[i], sep = " "))
}

pdf("figure/temp_gaussian.pdf")
multiplot(plotlist = plots, cols = 4)
dev.off()

#Make plots with varying kernels
#store them in a list and plot them all
plots <- list()
kernels <- c("gaussian", "rectangular", "triangular",
             "epanechnikov", "biweight", "cosine" , "optcosine")

for (k in 1:length(kernels)){
      plots[[k]] <- ggplot(redwood_all, aes(x = temp)) +
                    geom_density(kernel = kernels[k], bw = 0.5) + 
                    theme_bw() + ylab("") + xlab("") +
                    scale_x_continuous(limits = c(0,40)) +
                    ggtitle(kernels[k])
}

pdf("figure/temp_ker.pdf")
multiplot(plotlist = plots,cols=4)
dev.off()

@

\begin{figure}[!h]
  \centering
	\includegraphics[width=\textwidth,height=10cm]{temp_Gaussian.pdf}
	\caption{Densities with a Gaussian kernel and various densities}
\end{figure}

\begin{figure}[!h]
  \centering
	\includegraphics[width=\textwidth,height=10cm]{temp_ker.pdf}
	\caption{Density with various kernels and a bandwidth of 0.1}
\end{figure}

We can clearly see how the plot change as we change the bandwidth. As the bandwidth increases, the curve goes smoother and smoother. A bandwidth of 0.5 is probably the most appropriate here to represent the data (which is is roughly what the automatic setting chooses). Changing the kernels, however, doesn't seem to have any impact on the curve here, probably because of the high number of points being represented for any given value.

\FloatBarrier

\section{Temperature versus humidity}

<<Humid,echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide',cache=TRUE>>=
#We have 288 time points per day so
#Epoch modulo 288 selects the same time of the day for every day

redwood_all <- redwood_all %>% filter(epoch %% 288 == 3018 %% 288)

#Make plots with varying spans (bandwidth)
#store them in a list and plot them all

spans <- c(0.1,0.5,1,10)
plots <- list()
for (i in 1:length(spans)){
  
  plots[[i]] <- ggplot(redwood_all, aes(x = temp,y = humid_adj)) + 
                geom_point(alpha = 0.1) +
                geom_smooth(se = FALSE, method = "loess",span = spans[[i]]) +
                ylab("") + xlab("") + theme_bw() +
                ggtitle(paste("bw", spans[i], sep = " "))
}

pdf("figure/humid_bw.pdf")
multiplot(plotlist = plots,cols = 2)
dev.off()


#The for loop doesn't work with geom_smooth, it fits the 
#same curve to every plot so we store each plot individually
#with varying degrees

plots<-list()

plots[[1]]<-ggplot(redwood_all, aes(x = temp,y = humid_adj)) + 
            geom_point(alpha = 0.1) +
            geom_smooth(se = FALSE, method = "loess", formula = y~poly(x,1), span = 1) +
            ylab("") + xlab("") + theme_bw() +
            ggtitle(paste("degree", 1, sep = " "))

plots[[2]]<-ggplot(redwood_all, aes(x = temp,y = humid_adj)) + 
            geom_point(alpha = 0.1) +
            geom_smooth(se = FALSE, method = "loess", formula = y~poly(x,2), span = 1) +
            ylab("") + xlab("") + theme_bw() +
            ggtitle(paste("degree", 2, sep = " "))

plots[[3]]<-ggplot(redwood_all, aes(x = temp,y = humid_adj)) + 
            geom_point(alpha = 0.1) +
            geom_smooth(se = FALSE, method = "loess", formula = y~poly(x,3), span = 1) +
            ylab("") + xlab("") + theme_bw() +
            ggtitle(paste("degree", 3, sep = " "))

plots[[4]]<-ggplot(redwood_all, aes(x = temp,y = humid_adj)) + 
            geom_point(alpha = 0.1) +
            geom_smooth(se = FALSE, method = "loess", formula = y~poly(x,4), span = 1) +
            ylab("") + xlab("") + theme_bw() +
            ggtitle(paste("degree", 4, sep = " "))


pdf("figure/humid_degree.pdf")
multiplot(plotlist = plots,cols = 2)
dev.off()

#Remove all variables so that they don't stay
#in the environment
rm(redwood_all,degrees,plots,spans,i,cleanDatesData,cleanRedwoodData,loadDatesData,loadMoteLocationData,loadRedwoodData)
@


\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.45\textwidth}
  	\includegraphics[width=\textwidth]{humid_bw.pdf}
	  \caption{Loess fitting with varying spanning values and degree 1}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
	  \includegraphics[width=\textwidth]{humid_degree.pdf}
	  \caption{Loess fitting with various polynomials and span 1}  
  \end{subfigure}
\end{figure}


In that case, we can clearly see the impact of changing the spanning value (the bandwidth) on the fit. Smaller spanning values mean a closer fit to the data but the fit is also less smooth - and we probably have a lot of bias. On the other hand, a too high spanning value is just a polynomial fit on the points.\\

When varying polynomial degrees, we have a similar effect. Fitting with a higher degree polynomial allows for a closer fit to the data but the curve is less smooth. On a side note, the loess fit gets extreme when you increase the polynomial degree without specifying a spanning value (not shown). The formula used by R to compute the optimal bandwidth only applies for a linear polynomial local fit.

\FloatBarrier

\part{Linguistic Data}
\setcounter{section}{0}

<<load data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
setwd("~/Documents/Stat215a/Projects/lab2/")
path <- "data/"
lingData <- read.table(paste0(path, "lingData.txt"), header = TRUE, stringsAsFactors = FALSE)
linglocation <- read.table(paste0(path, "lingLocation.txt"))
load(paste0(path, "question_data.Rdata"))
@

<<clean data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
#We only consider the questions 50 to 121 so we can filter the 
#all.ans table and the quest.mat table
quest.mat <- quest.mat %>% filter(qnum %in% 50:121)
all.ans <- all.ans[50:121]
@

<<Plotting parameters, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
#Load the states coordinates and add the state ID
state_df <- map_data("state")
state_id <- cbind(state.name,state.abb)
state_df <- state_df %>% filter (region != "district of columbia") %>%
                         arrange(region) %>%
                         mutate(region = gsub("(^|[[:space:]])([[:alpha:]])",
                                "\\1\\U\\2", region, perl=TRUE))

state_df <- state_df %>%   group_by(region) %>%
                           mutate(STATE = state_id[state.name==unique(region)[1],2],
                                  long = as.numeric(as.character(long)),
                                  lat=as.numeric(as.character(lat))) %>%
                           ungroup()

#Make a nice theme for plotting maps
blank_theme <- theme_bw() +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank()) 

#Group states in 3 regions
Region <- function(STATE){
  if (STATE %in%  c("TX","LA","MS","AL","GA","SC","TN")){return ("South")}
  if (STATE %in%  c("WI","MI","IL","IN")){return ("Greater Lakes")}
  if (STATE %in%  c("PA","NY","MA","RI","CT","NJ","VT","NH","ME")){return ("North East")}
  else return ("None")
}
@

\section{Introduction}

The study of dialects has long been of interest to linguists. In particular, the differences of dialects along geographical differences - or dialectometry - helps to explain many variations of local speeches and can inform or validate historical hypothesis on population movement and interactions. While this field has drawn a lot of attention, it has only recently started to use computational methods to better aggregate the vast amount of data collected. This data mainly constitutes of answers to questions asked by an interviewer. Before, the linguist could decide which questions he thought were the most significant and relevant in classifying dialects. Computational methods can avoid that subjective step and point to questions that actually differentiate between different dialects.


\section{The Data}

Here, the subject of interest is dialects in the English-speaking population of the US in 2003. 67 questions are considered, aimed at exploring which word would be use in a specific situation. Answers are to be chosen among a list, with an "other" option available. Questions includes \textit{What do you call the insect that flies around in the summer and has a rear section that glows in the dark?} or \textit{What do you call a point that is purely academic, or that cannot be settled and isn't worth discussing further?}. Answers for the latter questions would be \textit{a moot point}, \textit{a mute point}, \textit{either one of the above}, \textit{I have no idea} and \textit{other}. The position of each individual is known through their zip-code, city and state (self-reported). The aim of this study is to find relationships between location and specific answers to questions

\subsection{Data quality and cleaning}

First, there are only \Sexpr{dim(quest.use)[1]} questions that are of interest rather than all questions between 50 and 121 (some are related to pronunciations) so they are removed. Then some respondents didn't answer some questions some we remove them. \\

<<reduce data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
#We only take questions in quest.mat
all.ans <- all.ans[quest.use[,1]-49]

#We can check that qust.mat and quest.use are identital on those questions
c <- inner_join(quest.use, quest.mat, by = c("qnum", "qnum"))
sum(c$quest.x == c$quest.y) == 67

#0 is equivalent to no answer and we filter lines with half of zeros

ling <- lingData %>% select(5:71)
ling <- (rowSums(ling == 0) > 0)
lingData <- lingData[!ling,]
rm(c,ling,quest.mat)
@

<<remove NAs and keep only mainland US state, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
#Remove NAs
lingData <- lingData[rowSums(is.na(lingData)) == 0,]

#Keep only mainland states
lingData <- lingData %>% filter(STATE %in% state.abb) %>%
                         filter(STATE != "AK", STATE != "HI") %>%
                         arrange(STATE)

#Remove incoherent points
#Check whether points coordinates fall within the state ranges
Geography <- matrix()
ling <- matrix(,ncol=73)
for (ID in unique(state_df$STATE)) {
  
     inside <-in.out(as.matrix(state_df %>% filter(STATE == ID) %>% select(long, lat)),
                     as.matrix(lingData[lingData$STATE==ID, c("long","lat")]))
     ling <- rbind(ling,as.matrix(lingData[lingData$STATE==ID,]))
     Geography <- rbind(Geography, matrix(inside, ncol=1))
     }
Geography <- Geography[-1,]
ling <- ling[-1,]

#Filter the points and reconvert the data frame
lingData <- as.data.frame(ling[Geography,],stringsAsFactors = FALSE)
for (i in 5:73){
  lingData[,i] <- as.numeric(as.character(lingData[,i]))
  }
for (i in 1:4){
  lingData[,i] <-as.character(lingData[,i])
}

rm(i,ID,ling)

@

There are also some missing values for State and City, which in turn mean some missing values for longitudes and latitudes coordinates. There are \Sexpr{sum(rowSums(is.na(lingData))>0)} such respondents and they are filtered out as well. All respondents with non-existing state name are also filtered. \Sexpr{sum(!Geography)} points have coordinates that don't map their states. They are also removed. Finally, for plotting purposes, the states of Alaska (AK) and Hawaii (HI) will not be considered.\\

We can plot the resulting map and check that there doesn't seem to be any anomalies anymore in Fig 4a.

<<Respondant map, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
png("figure/map.png")
ggplot(lingData,
       aes(x = long, y = lat, col = STATE)) + 
      geom_point(alpha = 0.4) + blank_theme +
      theme(legend.position = "none") +
      geom_polygon(aes(x = long, y = lat, group = group),
                   data = state_df, colour = "black", fill = NA)
dev.off()                    
@
\subsection{Exploratory Data Analysis}

To get a better feel of the data and experiment with visualization, two questions are chosen and their relationship is studied in deeper depth. For this analysis, the focus will be on Q87 \textit{Do you use the term 'bear claw' for a kind of pastry?} and Q70 \textit{	
What do/did you call your maternal grandfather??}.\\

<<Select the data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE>>=
#Select only questions Q070 and Q087
ques <- lingData %>% select(CITY, STATE, ZIP, ID, Q087, Q070, long, lat)

#Remove those who didn't answer to either question
ques<-ques %>% filter(Q087 != 0 & Q070 != 0)
@

Those two questions are selected and individuals that didn't answer to one or the two questions, so \Sexpr{sum(rowSums((select(lingData,Q087,Q070))==0)>0)} respondents are taken out. 

<<Frequencies, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
pdf("figure/mosaic_plot.pdf")
mosaicplot(table(ques %>% select(Q087, Q070)),main = "")
dev.off()
@

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth,height=6cm]{map.png}
	\caption{Visualization of all respondents colored by state.}
	\end{subfigure}
  \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{mosaic_plot.pdf}
	  \caption{Relative frequencies of each pair of answer}
	\end{subfigure}
	\caption{}
\end{figure}

In Fig 4b, a few things are of note. Firstly, all answers are far from being equally likely. For Q118. answers 2,3,6 and 7are far more likely. There is also some relations. People who answered 2 ("no, but I know what it means") at Q87 are more likely to answer with 7 ("other") at Q70.\\

A $\tilde{\chi}^2$ analysis with $(7-1)\times (3-1)=12$ degrees of freedom allows to check for whether the samples are independent. The $\tilde{\chi}^2$ equals $234.05$, the answers to the questions are not independent.\\

\FloatBarrier

Reducing dimensions is not relevant here and there are already 21 natural clusters based on all possible answers. A few respondents had impossible state names (94 or XX) and they are also removed. Fig 5 gives a visualization of those points on a map of the US.

<<Map 2 questions, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
#Create clusters and count the numbers of occurences
ques<-ques %>% mutate(cluster = paste("Q70:", Q070, " and Q87:", Q087,sep = "")) %>%
               select(-Q070, -Q087, -CITY, -ZIP)

ques<-ques %>% group_by(cluster) %>%
               mutate(g = n()) %>%
               ungroup()

#plot the map for all clusters
png("figure/map2Q_tot.png")
ggplot(ques, aes(x = long, y = lat, col = cluster)) +
             geom_point(alpha = 0.4) + blank_theme +           
             theme(legend.position = "none") +
             geom_polygon(aes(x = long, y = lat, group = group),
                          data = state_df, colour = "black", fill = NA)
dev.off()

#plot the map for the smallest clusters
png("figure/map2Q_small.png")
ggplot(ques %>% filter(g < 500), aes(x = long,y = lat,col = cluster)) +
                                 geom_point(alpha = 0.4) +
                                 blank_theme + theme(legend.position = "none") +
                                 geom_polygon(aes(x = long, y = lat, group = group),
                                              data = state_df, colour = "black",
                                              fill = NA)
dev.off()

#plot the map fot the middle clusters
png("figure/map2Q_middle.png")
ggplot(ques %>% filter(g < 4000 & g > 500),
         aes(x = long, y = lat, col = cluster)) +
         geom_point(alpha = 0.4) + blank_theme +
         theme(legend.position = "none") +
         geom_polygon(aes(x = long, y = lat, group = group),
                      data = state_df, colour = "black", fill = NA)
dev.off() 

#plot the map fot the biggest clusters
png("figure/map2Q_big.png")
ggplot(ques %>% filter(g > 4000),
         aes(x = long, y = lat, col = cluster)) +
         geom_point(alpha = 0.4) + blank_theme +
         theme(legend.position = "none") +
         geom_polygon(aes(x = long, y = lat, group = group),
         data = state_df, colour = "black", fill = NA)
dev.off()  

@


Plotting all the possible combinations of answers, as in Fig. 5a, is not very meaningful for 2 reasons. One is that it is hard to find 21 really distinct colors so some clusters are colored in similar fashions even thought they may not be similar at all. Secondly, the biggest clusters can hide the smallest ones. Fig b-d show some subsets of respondents. \\

From Fig 5b, we can identify small but strongly localized clusters: one in Pennsylvania (in blue), one around New York (in brown) and one around Boston. If we look at the labels (not shown), we can see that they match rare answers to Q70. Some people from Pennsylvania call their granddad Pap, some from New York and Boston call theirs gramps. The difference between the New York and the Boston clusters are their answer to Q87. \\

From Fig 5c, we can see the emergence of two regions in the east of the US, even though it is hard to be sure. Fig 5d marks the distinction more clearly by focusing on the 4 biggest clusters. East of -100 long, the US are divided along a line South-West to North-East. The main divider is again Q70. South of the Line, respondents use a different name for their granddad in private and in public whereas north of the Line, they use the same. The differences inside those 2 regions are either due to the name used (for the northern part) or to Q87. West of -100 long, it is hard to sport any particular trends.\\

Picking two questions to study in depth can be useful to get a better sense of the data. With only two questions, we can already see the emergence of regional trends. However, as could be expected, the most meaningful differences are based on the answers to the question with the most choices (here Q70).\\

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth,height=6cm]{map2Q_tot.png}
	\caption{Visualization of all respondents colored by their answers to Q70 and Q87}
	\end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth,height=6cm]{map2Q_small.png}
	  \caption{Subset of all respondents for rare answers}
	\end{subfigure}
	 \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth,height=6cm]{map2Q_middle.png}
	  \caption{Subset of all respondents for intermediate answers}
	\end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth,height=6cm]{map2Q_big.png}
	  \caption{Subset of all respondents for common answers}
	\end{subfigure}	
	\caption{Visualizations of respondents based on answers to Q70 and Q87}
\end{figure}

\FloatBarrier

\section{Dimension reduction methods}
\subsection{Binary format}

<<Binary, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=

#For all questions, put the question in binary format
BinData <- lingData
c <- 1
#Loop over all questions
for(i in quest.use$qnum){
  #Put the question in the right format
  if (i < 100){
    question <- paste("Q0", i,sep = "")
    } else{
  question <- paste("Q", i, sep = "")
    }
  #Goes through all possible answers for the question
  for (j in 1:dim(all.ans[[c]])[1]){
    answer <- paste(question, ".", j, sep = "")
    #Add the binary column
    BinData[,answer] <- as.numeric(BinData[,question] == j)
  }
  c <- c + 1
} 
#Remove the original columns and variables
BinData <- BinData %>% select(-(5:71))
rm(c, i, j, question, answer)
@

<<Training/Test, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
#Draw the test set
test<-BinData %>% group_by(STATE) %>% 
                  sample_frac(size = 0.3) %>%
                  ungroup()

#Keep the remainder as training set
training <- BinData %>% filter(!ID %in% test$ID)
@

To reduce the dimensions of the data set, it needs to be put in binary format. Then, we sampled 70\% of the sample from each state and keep the rest as a test set. Sampling among state ensure that the proportion of respondent by states are conserved. Otherwise, since some states have few respondents, our test might be to sensitive to the sampling.\\

\subsection{PCA}

<<PCA, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
#Run PCA on the answers
pca <- prcomp(training[,7:474])

#Most important questions
ques <- as.matrix(pca$rotation) %*% diag(pca$sdev)
ques <- rowSums(ques[,1:42])
sort(abs(ques))

#Plot the cumulative distribution of the PCs
pc <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
pc <- as.data.frame(cbind(1:468, pc))
colnames(pc)<-c("Index","Cum_Sum")

pdf("figure/cum_sum.pdf")
ggplot(pc, aes (x = Index, y = Cum_Sum)) + geom_point(size=0.1) + 
          theme_bw() + ylab("Cumulative sum of Eigenvalues") + 
          geom_abline(slope = 0, intercept = 0.9, col = "red") +
          geom_abline(slope = 0, intercept = 0.5, col= "blue") +
          geom_abline(slope = 0, intercept = 0.25, col = "orange")
dev.off()
@

<<PCA results, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,results='hide'>>=
#Build a data frame with the 2 first PCs and the labels
pcs <- cbind(as.matrix(training[,1:6]), pca$x[,1:2])
pcs <- as.data.frame(pcs, stringsAsFactors = FALSE)
pcs <- pcs %>% mutate(pc1=as.numeric(as.character(PC1)),
                      pc2=as.numeric(as.character(PC2)),
                      long = as.numeric(as.character(long)),
                      lat=as.numeric(as.character(lat)))

#Build the average values
pcs <- pcs %>% group_by(STATE) %>% 
              mutate(PC1 = mean(pc1), PC2 = mean(pc2),
                     LAT = mean(lat), LONG = mean(long))

#General plot by plotting all points
png("figure/PCA_allPoints.png")
ggplot(pcs, aes(x = pc1, y = pc2, col = STATE)) + 
            geom_point(alpha = 0.4, size=0.5)  + theme_bw() +
            theme(legend.position = "none")
dev.off()

#Plot the mean PCs against the mean values
pdf("figure/PCA_mean.pdf")
ggplot(pcs, aes(x = PC1, y = LONG, col = STATE)) + 
            geom_point() + theme_bw() +
            theme(legend.position = "none") 
dev.off()

#Filter out western states and Florida
pdf("figure/PCA_long.pdf")
ggplot(pcs %>%filter(LONG>-100, STATE!="FL"), 
       aes(x = PC1, y = LONG)) +
       geom_point() +
       geom_smooth(se=FALSE, method  = "lm") + theme_bw()
dev.off()

pdf("figure/PCA_lat.pdf")
ggplot(pcs %>%filter(LONG>-100, STATE!="FL"), 
       aes(x = PC2, y = LAT)) +
       geom_point() +
       geom_smooth(se=FALSE, method  = "lm") + theme_bw()
dev.off()

#Transform and fit the map on the points
pc_reduc <- pcs %>%filter(LONG>-100, STATE!="FL")
lm_pc1 <- lm(pc1 ~ long + lat, pc_reduc)
lm_pc2 <- lm(pc2 ~ long + lat, pc_reduc)

state_df_pcs <- state_df %>% filter(STATE %in% pc_reduc$STATE) %>%
                mutate(
                long = predict.lm(lm_pc1,newdata = data.frame(long = long, lat = lat)),
                lat = predict.lm(lm_pc2,newdata = data.frame(long = long, lat = lat))) %>%                      mutate(long = long -mean(long), lat =lat - mean(lat))


png("figure/PCA_map.png")
ggplot(pc_reduc, aes(x = pc1, y = -pc2, col =STATE)) +
                 geom_point(alpha=0.4,size=0.5) +blank_theme +           
             theme(legend.position = "none") +
             geom_polygon(aes(x = long, y = -lat, group = group),
                          data = state_df_pcs, colour = "black", fill = NA)
dev.off()

#Plot for a subest of states
pc_reduc <- pc_reduc %>% mutate(region = Region(STATE))

pdf("figure/PCA_select.pdf")
ggplot(pc_reduc %>% filter (region != "None"),
                 aes(x = pc1, y = -pc2, col = region)) +
                 geom_point(alpha=0.4, size=1) + blank_theme +
                 geom_polygon(aes(x = long, y = -lat, group = group),
                              data = state_df_pcs, colour = "black", fill = NA)+
                 scale_color_manual(
                 values = c("chartreuse4", "deepskyblue","deeppink"), name = "States") +
       guides(colour = guide_legend(override.aes = list(size=2, alpha=1)))
dev.off()
@

Afterward, Principal Component Analysis is run. As we can see in Figure 6a, the first 155 PC explain 90\% of the variance in the data, the first 42 PC explain 50\% of the variance, the first 13 explain 25\%. The most important answers based on the first 42 PCs are 72.1 and 72.5. Question 72 is \textit{What do you call the big clumps of dust that gather under furniture and in corners?} and the answers are \textit{dust bunnies}, \textit{dust kittens}, \textit{dust mice}, \textit{kitties}, \textit{dust balls} and \textit{other}.\\

The aim is to see if the geographical distances can be recovered from the distances computed with answers. Here, the implicit distance chosen when using PCA is 1-correlation. Therefore, the points are plotted according to their 2 first PCs in figure 6b. Since there are some many points, it is hard to see much, so mean values are considered. \\

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{cum_sum.pdf}
    \caption{Cumulative sums of the PCs,\\ with cutoffs at 0.25, 0.5 and 0.9}
	\end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{PCA_allPoints.png}
	  \caption{Plotting of the 2 first PCs for all points}
	\end{subfigure}
  \caption{Principal Component Analysis}
\end{figure}

After grouping points per states, the average PC1 is plotted against the average longitude (Fig 7a) and latitude (not shown) to see if there is any correlation. As can be seen on Figure 7a, the mean of first PC seems really linked to the mean longitude for longitudes greater than -100, and excluding Florida. This can be understood since those states are much more recent and may not have time to develop local dialects. The origin of the respondents is more relevant than there location. Filtering out those states lead to Fig 7b and 7c. There seems to be a clear relation between PCs and geographical coordinates. 
\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{PCA_mean.pdf}
	  \caption{Mean longitude versus mean PC1, per state}
	\end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{PCA_long.pdf}
	  \caption{Mean longitude versus mean PC1, per state east of-100 long, excluding Florida}
	\end{subfigure}
	 \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{PCA_lat.pdf}
	  \caption{Mean latitude versus mean PC2, per state east of -100 long, excluding Florida}
	\end{subfigure}
\end{figure}


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth,height=8cm]{PCA_map.png}
  \caption{Plotting of the 2 first PCs for all points, with the transformed map added on top}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth,height=8cm]{PCA_select.pdf}
	 \caption{Plotting of the 2 first PCs, for 3 states}
\end{figure}

It is therefore possible to fit linear regressions on PC1 and PC2, based on longitude and latitudes. This can be used to transform the us map and plot it on top of the points, on Fig 8. However, most points actually overflow the map. Therefore, to get a better sense of whether points really cluster by location, only a subset of the points corresponding to well-known regions are plotted, and colored by region. The function \textit{Region} in the code gives a More precise list of which state is in which region.\\

On Fig 9, the relative positions of the regions are well-respected and the Boundaries are quite clear, even if some points cluster with the wrong labels. What is also apparent is that, even though all labelled clusters cover their assigned states, most of the points are outside the map. This can be expected since the distance between dialect probably don't evolve linearly with physical distances, but probably with varying paces depending on the place, the population density, and soon and so forth. However, being able to recover the relative positions from the PCA and relatively fit the map can still be satisfactory.\\

It can also be expected that, even if we get the right relative positions, there won't be a perfect match with the distance. Also, only the first 2 PCs are used and they represent only a small fraction of the variance. Finally, using correlation as a distance might not be very relevant. 

\FloatBarrier

\subsection{Multi-dimensional scaling}

On the binary matrix, an euclidean distance is just the square root of the sums of dissimilar answers minus the average number of answers Using multi-dimensional scaling, this distance matrix can be projected on two dimensions.\\

Computing a distance matrix on the full training set is not computationally feasible (it would be a \Sexpr{dim(training)[1]} $\times $ \Sexpr{dim(training)[1]} matrix so it weight more than 10GB) so only a subset of the points is considered. Sampling is done by state so that the the repartition of the data remain identical.\\

<<MDS, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Reduce the size and define the distance matrix
reduc<- training %>% group_by(STATE) %>%
                     sample_frac(size=0.1)
reduc_dist <- as.matrix(reduc[,7:474])
reduc_dist <- dist(reduc_dist)
reduc_dist <- as.matrix(reduc_dist)
n <- nrow(reduc_dist)

# define centering matrix J
I <- diag(1, n)
one <- matrix(1, nrow = n, ncol = n)
J <- I - (1 / n) * one
rm(I,one,n)

# Apply double centering
reduc_dist <- -(1 / 2) * (J %*% (reduc_dist)^2 %*% J)

# obtain eigendecomposition of B
eigen_dist <- eigen(reduc_dist)
Eigen <- Re(eigen_dist$vectors[, 1:2])
Lambda <- Re(diag(eigen_dist$values[1:2]))


# define projected X
X <- Eigen %*% sqrt(Lambda)
@

<<MDS plots, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=

#Combine the mds values with the labels
#Arrange the data
mds_reduc<-cbind(as.matrix(reduc[,1:6]), X[,1:2])
colnames(mds_reduc)[7:8]<-c("x1","x2")
mds_reduc <- data.frame(mds_reduc, stringsAsFactors=FALSE)
for (i in 5:8){ mds_reduc[,i] <- as.numeric(mds_reduc[,i])}

mds_reduc <- mds_reduc %>% group_by(STATE) %>%
                           mutate(X1 = mean(x1), X2 =mean(x2),
                           LAT = mean(lat), LONG = mean(long))

#Filter the western states and Florida
mds_reduc_select <- mds_reduc %>%filter(LONG>-100, STATE != "FL")

#Build the regression to teransform the map
lm_x1 <- lm(x1 ~ long + lat, mds_reduc_select)
lm_x2 <- lm(x2 ~ long + lat, mds_reduc_select)

# Transform and center the map
state_df_mds <- state_df %>% filter(STATE %in% mds_reduc_select$STATE) %>%
                mutate(long = predict.lm(lm_x1,newdata = data.frame(long = long, lat = lat)),
                       lat = predict.lm(lm_x2,newdata = data.frame(long = long, lat = lat))) %>%
  mutate(long = long -mean(long), lat =lat - mean(lat))

#Plot the points and the centered map.
png("figure/MDS_map.png")
ggplot(mds_reduc_select, aes(x = -x1, y = -x2, col =STATE)) +
                 geom_point(alpha=0.4,size=2) +blank_theme +           
                 theme(legend.position = "none") +
                 geom_polygon(aes(x = -long, y = -lat, group = group),
                              data = state_df_mds, colour = "black", fill = NA)

dev.off()

#Plot the points and the centered map on selected states
mds_reduc_select <- mds_reduc_select %>% mutate(region = Region(STATE))

png("figure/MDS_map_select.png")
ggplot(mds_reduc_select %>% filter (region != "None"),
                 aes(x = -x1, y = -x2, col = region)) +
                 geom_point(alpha=0.4, size=4) + blank_theme +
                 geom_polygon(aes(x = -long, y = -lat, group = group),
                              data = state_df_mds, colour = "black", fill = NA)+
                 scale_color_manual(
                 values = c("chartreuse4", "deepskyblue","deeppink"), name = "States") +
       guides(colour = guide_legend(override.aes = list(size=2, alpha=1)))
dev.off()
@

This time, there also is a relationship between the two first dimensions of the MDS, and the longitude and latitudes of the points, on average, for the states east of -100 long excluding Florida (not shown). Therefore, as in the PCA analysis, the US map can be transformed and fitted to the data points which underwent multi-dimensional scaling. This is what is present in Fig. 10a and 10b, which are the equivalents of Fig 8 and 9. The points also overflow the fitted map but we can recover the geographical clusters.

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.4\textwidth} 
    \includegraphics[width=\textwidth]{MDS_map.png}
	  \caption{Plotting of the sampled points after multi-dimensional scaling, with the transformed map added on top}
	\end{subfigure}	
	\begin{subfigure}{0.55\textwidth}
    \includegraphics[width=\textwidth,height=7cm]{MDS_map_select.png}
	  \caption{Subset of (a) with a selection of states}
	\end{subfigure}
	\caption{Multi-dimensional scaling}
\end{figure}

\subsection{Comparison of the two techniques}

It is hard to know whether the two methods presented are very different since they are not performed on the same data set. Therefore, we only select the points that where used for both methods. \\

<<Comparison,  echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Build the common matrix
pc_reduc <- pc_reduc %>% select (-PC1, -PC2, -LAT, -LONG)
Common <- inner_join(pc_reduc, mds_reduc_select, by = c("ID" = "ID", "lat" = "lat", 
          "long" = "long", "STATE" = "STATE", "region" = "region", "ZIP" = "ZIP",
          "CITY" = "CITY"))
Common <- Common %>% ungroup() %>%filter (region != "None")

#Compute distance matrices
dist_pc <- dist(as.matrix(Common %>% select(pc1, pc2)))
dist_pc <- as.matrix(dist_pc)
dist_mds <- dist(as.matrix(Common %>% select(x1, x2)))
dist_mds <- as.matrix(dist_mds)

#Compute the respective silhouettes
#Need to name the clusters as integers
ToInt <- function(region){
  if (region == "Greater Lakes") { return (1)}
  if (region == "North East") { return (2)}
  if (region == "South") { return (3)}
}
Clusters <- unlist(lapply(Common$region,ToInt))

s_pc <- silhouette(x = Clusters, dmatrix = dist_pc)
s_mds <- silhouette(x = Clusters, dmatrix = dist_mds)
mean(s_pc[,3])
mean(s_mds[,3])
@

If the results from the previous PCA are used, the Multi-Dimensional Scaling method  and the PCA are nearly as efficient: the average silhouette length is \Sexpr{round(mean(s_mds[,3]), digits=3)} for MDS, while it is \Sexpr{round(mean(s_pc[,3]), digits=3)} for PCA. \\

<<Comparison 2, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Run the pca
reduc_tot <- reduc %>% mutate (region = Region(STATE)) %>%
                      filter (region != "None" )
pca_reduc <- prcomp(reduc_tot[,7:474])

#Run the MDS

dist_mds_reduc <- dist(reduc_tot[,7:474])
dist_mds_reduc <- as.matrix(dist_mds_reduc)
n <- nrow(dist_mds_reduc)
I <- diag(1, n)
one <- matrix(1, nrow = n, ncol = n)
J <- I - (1 / n) * one
dist_mds_reduc <- -(1 / 2) * (J %*% (dist_mds_reduc)^2 %*% J)
eigen_dist <- eigen(dist_mds_reduc)
Eigen <- Re(eigen_dist$vectors[, 1:2])
Lambda <- Re(diag(eigen_dist$values[1:2]))
X <- Eigen %*% sqrt(Lambda)

#Compute the distance
dist_pc_reduc <- dist(as.matrix(pca_reduc$x[,1:2] ))
dist_pc_reduc <- as.matrix(dist_pc_reduc)

dist_mds_reduc <- dist(X[,1:2])
dist_mds_reduc <- as.matrix(dist_mds_reduc)

s_pc_reduc <- silhouette(x = Clusters, dmatrix = dist_pc_reduc)
s_mds_reduc <- silhouette(x = Clusters, dmatrix = dist_mds_reduc)

mean(s_pc_reduc[,3])
mean(s_mds_reduc[,3])
@

If we run the PCA and the MDS specifically on the selected points, the average silhouette length is exactly the same! and is worth \Sexpr{round(mean(s_mds_reduc[,3]), digits=3)}. It is something that could have been expected. The PCA uses 1 minus correlation as a distance, while the MDS was run with euclidean distances. However, in the case of the binary data, it amounts to the same: the first distance counts the number of similar answers while the second counts the number of different answers. After scaling, those distances are therefore identical. Therefore, the two methods will give the same results in that specific case. 

\subsection{Clustering}

\begin{wrapfigure}{r}{0.5\textwidth} 
\vspace{-20pt}
  \begin{center}
	\includegraphics[width=0.4\textwidth]{silhouette.pdf}
	\caption{Average silhouette width for various number of cluster}
  \end{center}
  \vspace{-20pt}
  \vspace{1pt}
\end{wrapfigure} 

Running a clustering method on the whole training is not computationally feasible, since this also requires computing a distance matrix. Therefore, a sample of the respondents needs to be considered for clustering. Furthermore, running a spectral clustering method (PCA + k-mean) on all states yield unsatisfying results: the clusters are not stable. Therefore, the clustering will focus on the subset of states identified in the previous steps for clustering.\\

<<Clustering, get K, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Filter the data set
set.seed(0)
cluster_set <- as.matrix(training)
cluster_set <- as.data.frame(cluster_set, stringsAsFactors = FALSE)
for (i in 7:474){cluster_set[,i] <- as.numeric(cluster_set[,i])}
  
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               mutate(region = Region(STATE)) %>%
                               filter(region != "None")

#Get a sample of the data set
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               sample_frac(size=0.1)

pca <- prcomp(cluster_set[,7:474])
dist <- dist(pca$x[,1:42])
dist <- as.matrix(dist) 

k <- list()
s <- list()
centers<-2:10

#Run the k-man algorithm for various k
for(c in centers){
  k[[c-1]] <- pam(dist, k = c, diss = TRUE)
  s[[c-1]] <- silhouette(k[[c-1]]$clustering, dist)
}

# average silhouette width
means <- unlist(sapply(s, FUN=function(x){mean(x[,3])}))

s<-as.data.frame(cbind(2:10,means))
colnames(s)<-c("k","mean")
pdf("figure/silhouette.pdf")
ggplot(s, aes(x = k, y = mean)) + 
          geom_line() + ylab("Average silhouette length") +
          xlab("Number of clusters") + theme_bw() +
          geom_point(aes(col = mean>0.066 & k<5,
                         size = as.numeric (mean>0.066 & k<5))) +
          theme(legend.position = 'none') + scale_color_manual(values = c("black", "red"))
dev.off()
@

Using pam as a K-mean algorithm after computing a distance matrix using the first 42 PCs (that explain 42\% of the variance), the average silhouette width can be computed and plotted for various number of clusters. This can help to determine which k to choose. On Fig 11, k = 4 or k = 9 can be picked as clusters. Using Ockam razor, k = 3 is selected.\\

<<Clusters, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Get the cluster labels
k4 <- pam(dist, k = 4, diss = TRUE)
k8 <- pam(dist, k = 8, diss = TRUE)
cluster_set <- cbind(as.matrix(cluster_set[,c(1:6,475)]), 
                     matrix(k4$clustering, ncol = 1),
                     matrix(k8$clustering, ncol = 1),
                     pca$x[,1:2])

colnames(cluster_set)[8]<-"clusters_4"
colnames(cluster_set)[9]<-"clusters_8"

for (i in c(5,6,10,11)){ cluster_set[,i] <- as.numeric(cluster_set[,i])}
cluster_set <- as.data.frame(cluster_set, stringsAsFactors = FALSE)
for (i in c(5,6,10,11)){ cluster_set[,i] <- as.numeric(cluster_set[,i])}

pdf("figure/clusters.pdf")
ggplot(cluster_set, aes(x = long, y = lat, col= clusters_4)) + 
                    geom_point() + blank_theme +
                    geom_polygon(aes(x = long, y = lat, group = group),
                    data = state_df, colour = "black", fill = NA)+
                    scale_color_brewer(name = "Clusters", type = "qual")
dev.off()

print(xtable(table(cluster_set %>% select(clusters_4,region)), type = "latex"), file = "figure/filename2.tex")
@

On Fig 12, the clusters learned from the data do seem to partially match the geographic labels. The table of frequencies is:

\input{figure/filename2.tex}

It is clear that clusters are linked to geography but the relation is not at all straightforward. Working with more data points (and a more powerful computer) may help to better match geography and dialects.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.5\textwidth]{clusters.pdf}
	\caption{Average silhouette width for various number of cluster}
\end{figure}	

\section{Stability of findings to perturbation}

\subsection {PCA}

Several of the findings relies on random draws so it is easy to test for perturbations. The first perturbation to be studied is the PCA where the test set comes into play in two manners. First, the test set is projected along the 2 first PCs found before and is plotted along with the fitted map to check for consistencies (Fig 13a). Then, a PCA is run on the test set and another map is fitted so it is possible to compare the 2 maps (Fig 13b). The 2 figures show that our results are consistent between the training and the test set and are therefore quite resilient to perturbations.\\

This also validates the sampling method with grouping by stats before uniform random sampling, instead of straight uniform random sampling.\\

<<PCA stability 1, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Project the test set
pca <- prcomp(training[,7:474])
test_reduc <- test %>% group_by(STATE) %>%
                       mutate(region = Region(STATE)) %>%
                       filter(region != "None")
test_proj <- as.matrix(test_reduc)
test_proj <- as.numeric(test_proj[,7:474])
test_proj <- matrix(test_proj, ncol = (474-6))
rot<- as.matrix(pca$rotation)
test_proj <- test_proj %*% rot
test_proj <- test_proj[,1:2]

#Plot the projection
test_proj <- cbind(test_proj, matrix(test_reduc$region, ncol = 1))
colnames(test_proj)[3] <- "region"

test_proj <- as.data.frame(test_proj, stringsAsFactors = FALSE)

png("figure/stability_PCA.png")
ggplot(test_proj,
                 aes(x = as.numeric(PC1), y = -as.numeric(PC2), col = region)) +
                 geom_point(alpha=0.4, size=1) + blank_theme +
                 xlab("PC1") + ylab("-PC2") +
                 geom_polygon(aes(x = long, y = -lat, group = group),
                              data = state_df_pcs, colour = "black", fill = NA)+
                 scale_color_manual(
                 values = c("chartreuse4", "deepskyblue","deeppink"),
                 name = "States") +
       guides(colour = guide_legend(override.aes = list(size=2, alpha=1)))
dev.off()
@

<<PCA stability 2, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
pca <- prcomp(test[,7:474])
pcs <- cbind(as.matrix(test[,1:6]), pca$x[,1:2])
pcs <- as.data.frame(pcs, stringsAsFactors = FALSE)
pcs <- pcs %>% mutate(pc1=as.numeric(as.character(PC1)),
                      pc2=as.numeric(as.character(PC2)),
                      long = as.numeric(as.character(long)),
                      lat=as.numeric(as.character(lat)))

#Build the average values
pcs <- pcs %>% group_by(STATE) %>% 
              mutate(PC1 = mean(pc1), PC2 = mean(pc2),
                     LAT = mean(lat), LONG = mean(long))

#find the fitted map
pc_reduc <- pcs %>%filter(LONG>-100, STATE!="FL")
lm_pc1 <- lm(pc1 ~ long + lat, pc_reduc)
lm_pc2 <- lm(pc2 ~ long + lat, pc_reduc)

state_df_pcs_test <- state_df %>% filter(STATE %in% pc_reduc$STATE) %>%
                mutate(
                long = predict.lm(lm_pc1,newdata = data.frame(long = long, lat = lat)),
                lat = predict.lm(lm_pc2,newdata = data.frame(long = long, lat = lat))) %>%                      mutate(long = long -mean(long), lat =lat - mean(lat))

#2 maps
pdf("figure/2maps.pdf")
ggplot() + blank_theme +
           xlab("PC1") + ylab("PC2") +
           geom_polygon(aes(x = long, y = -lat, group = group),
                              data = state_df_pcs, colour = "blue", fill = NA)+
           geom_polygon(aes(x = -long, y = -lat, group = group),
                              data = state_df_pcs_test, colour = "black", fill = NA) 
dev.off()
@

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.4\textwidth} 
    \includegraphics[width=\textwidth]{stability_PCA.png}
	  \caption{Plotting of the test points after projection }
	\end{subfigure}	
	\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{2maps.pdf}
	  \caption{Fiited map for the training (blue) and the test (black) set}
	\end{subfigure}
	\caption{PCA robustness}
\end{figure}

\subsection{Clustering}

Another part where the findings depend on the sampling is the clustering since the number of clusters and the cluster themselves depend on the chosen sample. Picking different samples and plotting the average silhouette width for various number of clusters lead to Fig 14. Fig 14a - and even more 14b - lead to choices of different k and therfore different clusters (not plotted). As was expected from the preliminary work done on a sample from the whole dataset, the clusters are very unstable and sensitive to data perturbations.

<<Clustering seed 5, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Filter the data set
set.seed(5)
cluster_set <- as.matrix(training)
cluster_set <- as.data.frame(cluster_set, stringsAsFactors = FALSE)
for (i in 7:474){cluster_set[,i] <- as.numeric(cluster_set[,i])}
  
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               mutate(region = Region(STATE)) %>%
                               filter(region != "None")

#Get a sample of the data set
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               sample_frac(size=0.1)

pca <- prcomp(cluster_set[,7:474])
dist <- dist(pca$x[,1:42])
dist <- as.matrix(dist) 

k <- list()
s <- list()
centers<-2:10

#Run the k-man algorithm for various k
for(c in centers){
  k[[c-1]] <- pam(dist, k = c, diss = TRUE)
  s[[c-1]] <- silhouette(k[[c-1]]$clustering, dist)
}

# average silhouette width
means <- unlist(sapply(s, FUN=function(x){mean(x[,3])}))

s<-as.data.frame(cbind(2:10,means))
colnames(s)<-c("k","mean")
pdf("figure/silhouette5.pdf")
ggplot(s, aes(x = k, y = mean)) + 
          geom_line() + ylab("Average silhouette length") +
          xlab("Number of clusters") + theme_bw() +
          geom_point(aes(col = mean>0.052,
                         size = as.numeric (mean>0.052))) +
          theme(legend.position = 'none') + scale_color_manual(values = c("black", "red"))
dev.off()
@

<<Clustering seed 12, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, results='hide'>>=
#Filter the data set
set.seed(12)
cluster_set <- as.matrix(training)
cluster_set <- as.data.frame(cluster_set, stringsAsFactors = FALSE)
for (i in 7:474){cluster_set[,i] <- as.numeric(cluster_set[,i])}
  
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               mutate(region = Region(STATE)) %>%
                               filter(region != "None")

#Get a sample of the data set
cluster_set <- cluster_set %>% group_by(STATE) %>%
                               sample_frac(size=0.1)

pca <- prcomp(cluster_set[,7:474])
dist <- dist(pca$x[,1:42])
dist <- as.matrix(dist) 

k <- list()
s <- list()
centers<-2:10

#Run the k-man algorithm for various k
for(c in centers){
  k[[c-1]] <- pam(dist, k = c, diss = TRUE)
  s[[c-1]] <- silhouette(k[[c-1]]$clustering, dist)
}

# average silhouette width
means <- unlist(sapply(s, FUN=function(x){mean(x[,3])}))

s<-as.data.frame(cbind(2:10,means))
colnames(s)<-c("k","mean")
pdf("figure/silhouette12.pdf")
ggplot(s, aes(x = k, y = mean)) + 
          geom_line() + ylab("Average silhouette length") +
          xlab("Number of clusters") + theme_bw() +
          geom_point(aes(col = mean>0.07,
                         size = as.numeric(mean>0.07))) +
          theme(legend.position = 'none') + scale_color_manual(values = c("black", "red"))
dev.off()
@

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.3\textwidth} 
    \includegraphics[width=\textwidth]{silhouette5.pdf}
	  \caption{Sample 1}
	\end{subfigure}	
	\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{silhouette12.pdf}
	  \caption{Sample 2}
	\end{subfigure}
	\caption{Average silhouette width for various number of cluster}
\end{figure}

\FloatBarrier
\section{Conclusion}

Overall, it is quite possible to recover a good sense of the relative geographic positions of the points from their answers to the questions for some parts of the US with older histories. This classification is quite robust to perturbations of the data.\\

However, trying to define clusters is not possible in any stable manner. Spectral clustering is probably not suited to the task and a softer clustering technique that assigns probabilities of belonging to a cluster might be mor relevant.\\

The distance metric used here may also lack some precision. Here, every question is weighted the same. On the other hand, domain knowledge might indicate that some questions are more relevant thatn others. The fact that respondents could answer "other' (and that many did) is also a problem. A metric that doesn't take those answers into consideration might be more powerful as identifying clusters.\\


\end{document}