\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

%For headers
\usepackage{fancyhdr}
\pagestyle{fancy}

%For figures
\usepackage{graphicx}
\graphicspath{ {figure/} }
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage[list=true,listformat=simple]{subcaption}
\usepackage[justification=centering]{caption}

%For text and equations
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\setlength{\parindent}{0pt}

\setlength{\parskip}{\smallskipamount}

\begin{document}

\title{Final Project\\ Stat 215A, Fall 2017}

<<setup, include=FALSE>>=
set.seed(0)
library(knitr)
library(tidyverse)
library(stringr)
library(xtable)
library(stats)
library(foreach)
library(matrixStats)
library(doParallel)
library(corrplot)
library(glmnet)

#Create the figure folder
if (!file.exists("figure")){dir.create(file.path("figure"))}
@

\author{Hector Roux de B\'{e}zieux}

\maketitle

<<Load Data, echo =FALSE, message = FALSE, warning = FALSE>>=
# Load the data
load("data/fMRIdata.RData")

# Seperate the data into training/validation versus test set
set.seed(8)
rownames(fit_feat) <- paste("image", 1:1750, sep = "")
rownames(resp_dat) <- paste("image", 1:1750, sep = "")

# Get the test set
X_test <- fit_feat %>% sample_frac(size = 0.2)
Y_test <- resp_dat[rownames(X_test),]

# The training/validation set is the rest of the data
train_names <- setdiff(rownames(fit_feat), rownames(X_test))
X_train <- fit_feat[train_names,]
Y_train <- resp_dat[train_names,]

# Get the row indexes for partition between training and validation
validation <- sample(x = 1:nrow(X_train), size = nrow(X_train), replace = FALSE)
n <- length(validation) / 4
@

\begin{abstract}
Being able to understand how the brain encode and decode visual input has always been a challenge and a goal in neurology. A first aspect is being able to predict how different regions of the brain react when presented with different images. In this paper, we relied on fMRI of subjects looking at grey-scale pictures to build a predictive model of the brain response in the visual region. Using various model selection and regression tools, we chose the most appropriate model and studied how the spatial location of voxels are linked to their response to images. Our model performed especially well on voxels near the origin (center of the brain?). However, our model selection step was unstable, which limited the quality of our prediction methods. Further work would therefore require a more efficient way to select the best model.
\end{abstract}

\section{Introduction}

Deciphering how the brain respond to visual inputs, and in particular images, has long been a key interest in neuroscience. An image is viewed by the eye and becomes an electric signal which is then transmitted to the brain. Such a signal activates or repress neurons in certain regions of the human brain, depending on its content, light intensity and many other such features. Linking those image features to specific brain regions will therefore go a long way towards understanding how the human brain reacts to images. It is also a first step towards a more ambitious but fascinating goal: estimating the image seen by the person just by looking at its neurological response. \\

The focus of this lab is to develop a model that would properly estimate the response of several brain regions given an image. This paper approaches the problem through several steps: model selection, regression, diagnostic, interpretation and discussion of the model.

\section{Data description}

The data consists of a set of $1750$ gray-scale images that have been shown to a subject. Each image consists of $16384$ pixels ($128\times 128$). After transformation using Gabor wavelet pyramid, images are represented as vectors of length $10921$. As images are shown to the subject, the brain activity is recorded using functional magnetic resonance imaging or fMRI. This techniques discretizes the brain into cubes and measure the magnetic (and hence electric) activity of each of those regions. For our study, we consider 20 voxels that have been previously linked to visual functions. A more detailed description of the location of those regions will follow in part 5.\\

The data is therefore a $1750\times 10921$ X matrix of image features and a $1750\times 20$ Y matrix of voxel responses. A first separation done on the data is between a training/validation set and a test set. To do so, we randomly select $20\%$ of the data (i.e $20\%$ of the images and their associated response) to set out for a test set. This leaves us with 1400 images in the training/validation set and 350 in the test set.

\section{Methods}
\subsection{Setting the model with Lasso}

Because we have more covariates than observations, we are in a case of over-specification. Any modeling done using the full range of the predictors will result in obvious over-fitting. A method for selecting a reduced number of covariates needs to be used. In this paper, we use the Lasso regularization technique. While Ordinary Least Squares (OLS) find the $\hat{\beta}$ that minimizes $\sum_{i=1}^n \left(Y_i - X_i\beta\right)^2$, Lasso aims to minimize $\sum_{i=1}^n \left(Y_i - X_i\beta\right)^2 + \lambda |\beta|_1$, with $\lambda$ a parameter to choose. This will shrink $\hat{\beta}$ by setting more and more coefficients to zero as $\lambda$ increases. In practice, the algorithm selects an initial $\lambda$ big enough such as all coefficients are zero and then decreases $\lambda$ until we get close to over-specifying.

\subsection{Choosing $\lambda$}

To choose the value of $\lambda$, we used an objective function that we aimed to minimize. We considered 5 objective functions in total (see 3.4 for details). To obtain more robust results, we used 4-fold cross validation. The training/validation set is divided into 4 blocks (images are randomly assigned to each fold but the folds are the same for all voxels). One after the other, each block is left-out and used as validation set. The three others are used as training. On each training set, we run Lasso with a wide range of $\lambda$, starting with high values where all coefficients of $\hat{\beta}$ are zero and stopping when we have nearly as many non-zero coefficients as there are images in the three folds (1050). Then, we find the value of $\lambda$ such as the associated $\hat{\beta}$ minimizes each of the five objective function on the training set. We then only keep the covariates that have non-zero coefficients. 

\subsection{Regression}

Then using only those covariates, we can build a regression model on the training set constituted by the three folds. We use 2 regression models: usual OLS and ridge (where we minimize $\sum_{i=1}^n \left(Y_i - X_i\beta\right)^2 + \lambda (|\beta|_2)^2$). For Ridge, the value of $\lambda$ is chosen by 10-fold cross validation (picking the one that minimizes Mean Standard Error or MSE on the left-out fold). Then, we validate both those models on the left-out fold that acts as a validation set. We then measure 2 criterion for selecting the best model: MSE and the correlation between the fitted and real values.

\subsection{Objective functions}

As said above, we consider 5 objective functions:

\begin{itemize}
  \item Akaike Information Criterion (AIC). $AIC = -2\ell\ell(\hat{Y}) +2k$, with k the number of parameters in the model (i.e the numbers of covariates selected by the Lasso regularization). Here, by noting n the number of observations, $\ell\ell(\hat{Y}) = -\frac{n}{2}log(\sigma^2) + \frac{RSS}{\sigma^2} + C_1$, where $C_1$ is independent of the model. Since we do not know $\sigma^2$, we estimate it with $\hat{\sigma^2}=RSS$ so that $\ell\ell(\hat{Y})=-\frac{n}{2}log(RSS) + C_2$. Since we use the AIC to compare between models, we can drop the constant and we have : $AIC = n\times log(RSS) +2k$.\\
  \item Bayesian Information Criterion (BIC) is similar to the AIC but with a different penalty for the dimension of the model. $BIC = n\times log(RSS) +log(n)\times k$.\\
  \item AICc is the corrected AIC for finite samples. It adds an extra penalty for model complexity : $AICc = n\times log(RSS) +2k + \frac{2k(k+1)}{n-k-1}$\\
  \item Cross-validation (CV): we break the training set of three folds into 10 smaller folds. We leave out one fold at a time. On the nine other folds, we fit a Lasso regression and then compute the MSE on the tenth fold. We pick the $\lambda$ which has minimal average MSE.\\
  \item Estimation Stability with Cross Validation (ESCV) also relies on a 10-fold cross-validation but here we pick the $\lambda$ greater than the one picked by CV which also minimizes $\frac{Var(\hat{Y}(\lambda))}{||\text(mean)(\hat{Y}(\lambda))||_2^2}$. Sometimes, the $\lambda$ for CV and ESCV may well be the same (as is the case with other objective functions).
\end{itemize}

\subsection{Global model selection method}

In total, we have:

\begin{equation*}
\boxed{(20\text{ Voxels})\times(4\text{ Folds})\times(5\text{ Objective Functions})\times(2\text{ Models}) = 800\text{ Measures for 2 criteria}}
\end{equation*}

\subsection{Remarks}

\begin{itemize}
  \item Computing all this takes $\sim 50$ minutes with 10 cores on the clusters. The code used to produce such a result can be found in \textbf{R/parallel.R}. The output is the file \textbf{data/output.RData}
  \item The choice of 4 global folds was also done for practical computational purposes (the more folds, the longer it takes to run).
  \item We always impose that at least one coefficient be non-zero so that the regression can always happens, even though a model with zero coefficient (fitting $Y_i$ with $\bar{Y}$) sometimes minimizes the objective function even more.
\end{itemize}

\section{Results of model selection}

<<Extract Results, echo =FALSE, message = FALSE, warning = FALSE>>=
load("data/output.RData")

Extract_MSE <- function(CV_set){
  return(lapply(CV_set, '[[', "MSE"))
}

Extract_Corr <- function(CV_set){
  return(lapply(CV_set, '[[', "Corr"))
}

MSE <- lapply(model_selection, FUN = Extract_MSE)
Corr <- lapply(model_selection, FUN = Extract_Corr)

Bind_CV_sets <- function(CV_set){
  df <- do.call("rbind", CV_set)
  df <- cbind(df, floor(2:9/2))
  df
}

# Change the result form the list format of foreach to a dataframe
MSE <- lapply(MSE, Bind_CV_sets)
MSE <- do.call("rbind", MSE)
MSE <- cbind(MSE, floor(0:159/8 + 1), rownames(MSE))
colnames(MSE)[6:8] <- c("CV_set", "Voxel", "Model")
MSE <- data.frame(MSE)
MSE <- MSE %>% gather(key = "Measure", value = "Mse", 1:5)
MSE <- MSE %>% mutate(Mse = as.numeric(Mse))

Corr <- lapply(Corr, Bind_CV_sets)
Corr <- do.call("rbind", Corr)
Corr <- cbind(Corr, floor(0:159/8 + 1), rownames(Corr))
colnames(Corr)[6:8] <- c("CV_set", "Voxel", "Model")
Corr <- data.frame(Corr)
Corr <- Corr %>% gather(key = "Measure", value = "Cor", 1:5)
Corr <- Corr %>% mutate(Cor = as.numeric(Cor))

rm(model_selection, Bind_CV_sets, Extract_Corr, Extract_MSE)
@

<<Plots of results, echo =FALSE, message = FALSE, warning = FALSE>>=
MSE_plot <- ggplot(MSE %>% filter(Mse < 2000), aes(x = Measure, y = Mse,
                 col = as.factor(Voxel), shape = as.factor(CV_set))) + 
          geom_jitter(width = 0.2, show.legend = F, size = 3, height = 0) +
          theme(axis.text = element_text(size = 15),
                axis.title = element_text(size = 20)) +
          facet_wrap( ~ Model, ncol = 2) +
          labs(x = "Model Selection Criterion", y = "MSE")

ggsave(MSE_plot, file = "figure/Cor_plot.pdf")

Cor_plot <- ggplot(Corr, aes(x = Measure, y = as.numeric(Cor),
                 col = as.factor(Voxel), shape = as.factor(CV_set))) + 
          geom_jitter(width = 0.2, show.legend = F, size = 3, height = 0) +
          theme(axis.text = element_text(size = 15),
                axis.title = element_text(size = 20)) +
          facet_wrap( ~ Model, ncol = 2) +
          labs(x = "Model Selection Criterion",
               y = "Correlation between\n predicted and actual values")

ggsave(Cor_plot, file = "figure/Cor_plot.pdf", width = 7, height = 4, units = "in",
       scale = 1)
@

\subsection{Models performances}

\begin{figure}[!h]
  \centering
  \includegraphics[width = 0.8\textwidth]{Cor_plot.pdf}
  \caption{Correlation between the fitted and real values for all models, voxels (colors) and CV fold (shape)}
\end{figure}

We plot the results in Fig.1 . For each model and each objective function, we plot the correlation between fitted and real values. Each point corresponds to one voxel (colors) and one fold (shape). We can make several observations from that. The first is that, apart from AIC + OLS, all the models are fairly similar in their prediction power. We can compute the mean correlation and we get table 1.

<<Summary of results, echo =FALSE, message = FALSE, warning = FALSE>>=
# Condense the results by model 
Corr_sum <- Corr %>% group_by(Measure, Voxel, Model) %>% 
            mutate(mean_cor = mean(Cor)) %>%
            group_by(Measure, Model) %>% 
            summarise(mean_corr = mean(mean_cor), max_corr = max(mean_cor)) %>%
            arrange(desc(mean_corr))

MSE_sum <- MSE %>% group_by(Measure, Model) %>% 
             summarise(mean_mse = mean(Mse))

results <- inner_join(Corr_sum, MSE_sum,
          by = c("Model" = "Model", "Measure" = "Measure")) %>%
          arrange(desc(mean_corr)) %>% ungroup() %>%
          mutate(models = paste(Measure, Model, sep = " + "),
                 mean_mse = mean_mse/1050) %>%
          select(models, mean_corr, max_corr, mean_mse)

print(xtable(results, type = "latex",
      caption = "Mean Correlation, maximum correlation and mean MSE for each model",
             digits = 2, align = "cc|ccc"), file = "figure/Corr_results.tex",
      include.rownames = F)

Var_within <- Corr %>% group_by(Voxel, Model, Measure) %>%
              mutate(sd = sd(Cor)) %>%
              ungroup() %>%
              summarise(mean_sd = mean(sd))

Var_between <- Corr %>% group_by(Model, Measure, CV_set) %>%
               summarise(sd = sd(Cor)) %>%
               ungroup() %>%
               summarise(mean = mean(sd))
@

\input{figure/Corr_results.tex}

<<Select model, echo =FALSE, message = FALSE, warning = FALSE>>=
# Compute some metrics to help select the best models
# Compute the sd of the correlation and the mse
Sd_Corr <-Corr %>% filter(Model == "Ridge", Measure %in% c("AIC", "AICc")) %>%
          group_by(Model, Measure, Voxel) %>%
          mutate(sd = sd(Cor)) %>%
          group_by(Model, Measure) %>%
          summarise(mean_sd = mean(sd))

Sd_MSE <- MSE %>% filter(Model == "Ridge", Measure %in% c("AIC", "AICc")) %>%
          group_by(Model, Measure, Voxel) %>%
          mutate(sd = sd(Mse)) %>%
          group_by(Model, Measure) %>%
          summarise(mean_sd = mean(sd))
@

As could be seen in Fig 1, the worst model by far is the AIC + OLS. A little surprisingly, the best model is AIC + Ridge. Overall, the extra layer of regularization added by Ridge always improve performances. Also, our predictive power is still not optimal: the best correlation that we have is 0.5.\\

When we look more closely at the models, it can be seen that AIC tends to over-fit the training set by keeping too many parameters while BIC, CV and ESCV tend to under-specify the models: in some cases, only one or two covariates are kept. One possible explanation as to why those models are not optimal is that we choose the $\lambda$ that minimizes the MSE, plus a penalty for model complexity or some cross-validation, while our parameter of interest is the correlation. And, as can be seen from table 1, the ranking changes quite a lot between the two, even though the first two and the last models are shared.\\

\subsection{Selecting the best model}

As we can see the two best models are AIC + Ridge and AICc + Ridge but it is hard to break the tie based on the metrics in table 1. If we look at other metrics to select the best model, we can see that they are pretty consistent. The rankings in maximum correlation and minimum MSE are the same. The two best models are Lasso with AIC + Ridge and Lasso with AICc + Ridge. It is hard to select one model over the other with those metrics. One way to distinguish between them is to look at their consistency across voxels. For each of those models, we compute the standard deviation of the correlation across the 4 folds, for each voxel. The standard deviation for correlation is the same but not for the MSE (it is 22.18 for AIC + Ridge and 26.70 for AICc + Ridge). AIC + Ridge does seem the best model but since the differences are still tenuous, we will keep both of the models for further analysis.\\

\section{Results for each individual voxel}
\subsection{Quality of prediction}

Another result of interest is that there is much more difference between voxels than within voxels (for the 4 folds). The mean standard deviation of the correlation for each model and CV fold is \Sexpr{round(Var_between[1,1], digits = 2)} while the mean standard deviation for correlation for each voxel and model is \Sexpr{round(Var_within[1,1], digits = 2)}. This stability in our results is reassuring: the biological differences are stronger than the variations in our statistical models.\\

We can show the results for each voxel in our two best models in table 2. Depending on the voxel one considers, the best model might be either AIC + Ridge or AICc + Ridge. Moreover, as could be seen in Fig.1, the quality of our prediction varies wildly

<<Results of voxels for the two best models, echo =FALSE, message = FALSE, warning = FALSE>>=
#Get the mean correlation per voxel
Results_Voxels <- Corr %>%
                  filter(Model == "Ridge", Measure %in% c("AIC", "AICc")) %>%
                  group_by(Model, Measure, Voxel) %>%
                  summarize(correlation = mean(Cor)) %>%
                  ungroup() %>%
                  mutate(model = paste(Measure, Model, sep = " + ")) %>%
                  select(model, Voxel, correlation) %>%
                  spread(key = model, value = correlation) %>%
                  mutate(Voxel = as.integer(Voxel)) %>%
                  arrange(Voxel)

Results_Voxels_print <- cbind(Results_Voxels[1:10,], Results_Voxels[11:20,])

print(xtable(Results_Voxels_print, type = "latex",
      caption = "Mean Correlation for each voxel and each model",
             digits = 2, align = "crcc||rcc"),
      file = "figure/Results_Voxels.tex",
      include.rownames=FALSE)
@

\input{figure/Results_Voxels.tex}

\subsection{Link between quality of prediction and voxel characteristics}

To be able to explain why we can better predict some voxels than others, we need to look a bit deeper at the voxel first. Based on the responses to all images, we can use hierarchical clustering to make a dendogram on the voxels and establish several clusters. If we then plot the 3D-structure of the voxels and color each voxel by its cluster, we can clearly see, in Figure 2a, that statistical clusters based on responses to images are linked to spatial regions of the brain. The responses of the brain are clearly linked to the specific position of the voxels. We define descriptive names for the regions based on their position. The code to obtain the dendogram ans this plot can be found in \textbf{R/brain3D.R}.\\

We can also look at the quality of our predictors based on their spatial location on Fig 2b. We color the voxels based on the correlation of the fitted values with the real values, for the best model of each voxel (between the top two). There is a clear link between the spatial location and the precision of our models. The $z$-axis is the most linked to the precision but lower $x$ and $y$ are also associated with better fits. Overall, the closer the voxel is to $(0,0,0)$, the better the fit. 

\begin{figure}[!h]
  \centering
  \begin{subfigure}{0.48\textwidth}
  	\includegraphics[width=\textwidth]{brain.pdf}
	  \caption{Voxel identified based on their responses to the images}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
	  \includegraphics[width=\textwidth]{prediction_brain.pdf}
	  \caption{Voxel identified based on the quality of the fitted values}  
  \end{subfigure}
  \caption{Spatial location of the voxels}
\end{figure}

<<Characteristics of best voxels, echo =FALSE, message = FALSE, warning = FALSE>>=
Best_Results_Voxels <- Results_Voxels %>%
                       gather(key = "Model", value = "Cor", 2:3) %>%
                       group_by(Voxel) %>%
                       summarise(best_corr = max(Cor)) %>%
                       arrange(desc(best_corr))
@

\section{Diagnostics}
\subsection{Stability}

A first way to ensure stability has been discussed. Using 4-fold cross-validation: for each voxel and model, we have four measures for the correlation between the fitted and real values. As we have seen before, we can see that this correlation is quite stable across various folds, and that the differences between voxels are much more important.\\

Another measure of stability can be done on variable selection: we run a 5-fold cross-validation on the training/validation set. Such an analysis was run with \textbf{R/stability.R} and the output is \textbf{data/output.RData}. Using AIC and AICc, we find all selected covariates and we can compare between folds. We then count how many times each covariates is selected among the 5 folds and compute the frequencies of occurrences (from 0 to 1). We only show the results for 4 voxels with the highest fit on the validation set and not the zeros (they can be deduced quite easily from the table). As we can see in table 3, not many parameters are picked more than once or twice. Therefore, the selection of parameters is not very stable. The ridge regularization that happens after probably shrink all the parameters that are not common between the folds, which is why the final result is more stable. 

<<Stability, echo =FALSE, message = FALSE, warning = FALSE>>=
stability <- lapply(stability, data.frame)

# Exatrct the AIC measures
AIC <- lapply(stability, FUN = function(voxel){
  voxel[,seq(from = 1, to = 9, by = 2)]
})
AIC <- lapply(AIC, rowSums)
AIC <- data.frame(AIC)
colnames(AIC) <- paste0("voxel", 1:20)
AIC <- AIC %>% gather(key = "Voxel", value = "Occurences") %>%
               filter(Occurences > 0) %>%
              filter(Voxel %in% paste0("voxel", c(20, 10, 18, 12)))
AIC <- data.frame(table(AIC)) %>% spread (key = Voxel, value = Freq)
print(xtable(AIC, caption = "Frequencies at which  a parameter is picked",
              type = "latex", digits = 0, align = "cc|cccc"),
      file = "figure/AIC.tex", include.rownames = F)

# Extract the AICc measures
AICc <- lapply(stability, FUN = function(voxel){
  voxel[,seq(from = 2, to = 10, by = 2)]
})
AICc <- lapply(AICc, rowSums)
AICc <- data.frame(AICc)
colnames(AICc) <- paste0("voxel", 1:20)
AICc <- AICc %>% gather(key = "Voxel", value = "Occurences") %>%
               filter(Occurences > 0) %>%
              filter(Voxel %in% paste0("voxel", c(20, 10, 18, 12)))
AIcC <- data.frame(table(AICc)) %>% spread (key = Voxel, value = Freq)
@

\input{figure/AIC.tex}
\subsection{Validity of the fit}

Then we test out models on the test set. To so so, we use the full training/validation set to select the restricted covariates based on Lasso and choosing the appropriate $\lambda_{Lasso}$ with AIC and AICc. Then, we fit a Ridge regression to the training/validation set, choosing $\lambda_{ridge}$ with a 10-fold cross validation. Finally, we predict the values of the test set. We do that on the 4 voxels from before. Te code is in textbf{R/best\_models.R} and the results where too heavy to push to Github. In table 4, we can see that, for 3 out of 4 voxels, the results are very good and close to what we have on the validation set in 4.1. However, for voxel 20, the fit is now very poor. 

<<Validity,  echo =FALSE, message = FALSE, warning = FALSE, eval = F>>=

# Get the fitted value with AIC and AICc
Fitted <- lapply(validity, '[', "Fitted")
Fitted <- lapply(Fitted, '[[', 1)
Fitted <- data.frame(Fitted)
Fitted_AIC <- Fitted[, seq(from = 1, to = 7, by = 2)]
Fitted_AICc <- Fitted[,seq(from = 2, to = 8, by = 2)]
colnames(Fitted_AIC) <- colnames(Fitted_AICc) <- paste0("voxel", c(20, 10, 18, 12))

# Compute the correlation between fitted and real values on the test set
fit <- matrix(0, nrow = 2, ncol = 4)
colnames(fit) <- colnames(Fitted_AIC)
rownames(fit) <- c("AIC", "AICc")
for(i in 1:4){
  name <- colnames(Fitted_AIC)[i]
  fit[1, i] <- cor(Fitted_AIC[, name], Y_test[,name])
  fit[2, i] <- cor(Fitted_AICc[, name], Y_test[,name])
}

print(xtable(fit, caption = "Fit of our model on the test data",
              type = "latex", digits = 2, align = "c|cccc"),
      file = "figure/test.tex")
@

\input{figure/test.tex}

<<Voxel 20,  echo =FALSE, message = FALSE, warning = FALSE, eval = F>>=
# Find the common covariates
selected <- lapply(validity, '[', "selected")
selected <- lapply(selected, '[[', 1)
selected <- data.frame(selected)

Common <- matrix(0, ncol = 4, nrow = 3)
colnames(Common) <-  paste0("voxel", c(20, 10, 18, 12))
rownames(Common) <- c("Dimension with AIC", "Dimension with AICc",
                      "Common dimensions")
for (i in 1:4){
  Common[1,i] <- sum(selected[,2*i -1])
  Common[2,i] <- sum(selected[,2*i])
  Common[3,i] <- sum(rowSums(selected[,c(2*i-1,2*i)])==2)
}

print(xtable(Common, caption = "Dimensions of the model, per voxel",
              type = "latex", digits = 0, align = "c|cccc"),
      file = "figure/dim.tex")
@

We can look at the number of covariates selected for explanations in table 5. As mentioned before, AIC over-fits at first but Ridge regularization corrects that. However, for voxel 20, AICc only select 2 covariates. So there seems to be only a few powerful predictive covariates for voxel 20. Given how unstable the parameter selection is, it is therefore not surprising that the predictions are poor on voxel 2.

\input{figure/dim.tex}
\newpage
\section{interpretation}

We have already see which regions of the brains are more easily predicted than others. Now, we will look at which covariates are the most selected. We choose the predictors that are picked at least 8 times among the 5 folds from 6.1, and with AIC or AICc. The number of covariates that we select range from 0 for voxel 16 to 23 for voxel 18 so we have a lot of heterogeneity. There are 64 different covariates thus selected, and 11 covariates appear in at least 3 voxels. When looking at the list of covariates selected, we can notice that only one of them comes from the columns of X above 5000 and 15 from above 1000. However, this is just a coincidence. Randomly permuting the columns of X before re-doing the analysis deleted that phenomenon.\\

<<Selection, echo =FALSE, message = FALSE, warning = FALSE>>=
stability <- lapply(stability, rowSums)
high_occurences <- lapply(stability, FUN = function(parameters){
  which(parameters > 8)
})
@

<<Interpretation, echo =FALSE, message = FALSE, warning = FALSE, results='hide'>>=
cov <- as.integer(names(sort(table(unlist(high_occurences)), decreasing = T)))
source("R/utils.R")
pdf("figure/covariates.pdf")
par(mfrow = c(2, 3))
image(ReadRealBasisFunction(cov[1]), main = paste("Covariate", cov[1]))
image(ReadRealBasisFunction(cov[2]), main = paste("Covariate", cov[2]))
image(ReadRealBasisFunction(cov[3]), main = paste("Covariate", cov[3]))
image(ReadRealBasisFunction(cov[4]), main = paste("Covariate", cov[4]))
image(ReadRealBasisFunction(cov[5]), main = paste("Covariate", cov[5]))
image(ReadRealBasisFunction(cov[6]), main = paste("Covariate", cov[6]))
dev.off()
par(mfrow = c(1, 1))

Is_1<- lapply(high_occurences, FUN = function(parameters){
  sum(parameters == 1) ==1
})
which(unlist(Is_1))
@
\begin{figure}[!h]
  \centering
  \includegraphics[width = 0.8\textwidth]{covariates.pdf}
  \caption{Wave functions of the covariates}
\end{figure}

We can still, however, look at those covariates. For example, we can plot the top 6 covariates (in term of how many times we find them selected in the voxels), as in Fig. 3. We can notice some characteristics. Covariates 1546 and 1549 look similar, as do covariates 29, 88 and 94. We can also look at whether some results are coherent with the brain structure. The voxels where covariate 1549 matters are all the clusters from side 2 and center 1, from Fig 2a so we find once again a link with the neurology. However, the other covariates cannot be linked so easily to apparent clusters.


\section{Predicting the new data}

The response of the 20 voxels to the 120 new images was done as for the test set. The code used is in \textbf{R/prediction.R} and the results in \textbf{output/predv1\_hectorrouxdebezieux.txt}.

\section{Discussion}

We have establish several results in this paper: using Lasso for model selection and Ridge regularization for regression, we build a predictive model for the brain activity in 20 voxels of the visual region. We can see that voxel react similarly to spatially close voxels and that our model performs much better for some voxels than others (ranging from a correlation of 0 to 0.5). The correlation is relatively stable.\\

However, as we have seen, the model selection method is quite imperfect, even though Ridge regularization partly make up for that. In particular, the choice of covariates is not very stable with cross-validation. To mitigate this, the model-selection step could be stabilized with cross-validation. Parameters selected in k of the folds would be kept. k can be a bit too low because related over-fitting will be dealt with with Ridge regularization afterwards.\\

\end{document}